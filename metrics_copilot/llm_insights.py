"""
LLM-powered insight generation using OpenAI API.

This module enhances the statistical analysis with natural language insights
generated by large language models.
"""

import os
from typing import List, Dict, Any, Optional
from dataclasses import asdict
import json

try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

from .schemas import (
    AnalysisReport,
    Hypothesis,
    RecommendedDecision,
    NextCheck,
    TrendSummary,
    ChangePoint,
    ExperimentResult,
)


class LLMInsightGenerator:
    """Generate natural language insights using OpenAI's GPT models."""

    def __init__(self, api_key: Optional[str] = None, model: str = "gpt-4-turbo-preview"):
        """
        Initialize the LLM client.

        Args:
            api_key: OpenAI API key (if None, reads from OPENAI_API_KEY env var)
            model: Model to use (default: gpt-4-turbo-preview)
        """
        if not OPENAI_AVAILABLE:
            raise ImportError(
                "OpenAI package not installed. Install with: pip install openai"
            )

        self.api_key = api_key or os.getenv("OPENAI_API_KEY")
        if not self.api_key:
            raise ValueError(
                "OpenAI API key not found. Set OPENAI_API_KEY environment variable "
                "or pass api_key parameter."
            )

        self.client = OpenAI(api_key=self.api_key)
        self.model = model

    def enhance_hypotheses(
        self,
        report: AnalysisReport,
        max_hypotheses: int = 5
    ) -> List[Hypothesis]:
        """
        Generate enhanced hypotheses using LLM based on statistical findings.

        Args:
            report: Complete analysis report with all findings
            max_hypotheses: Maximum number of hypotheses to generate

        Returns:
            List of LLM-generated hypotheses with confidence scores
        """
        # Prepare context for LLM
        context = self._prepare_analysis_context(report)

        prompt = f"""You are a senior product analyst reviewing metrics data. Based on the statistical analysis below, generate {max_hypotheses} actionable hypotheses that explain what might be happening.

ANALYSIS CONTEXT:
{context}

For each hypothesis:
1. Provide a clear, specific explanation of what might be happening
2. Rate confidence as "high" (>70% likely), "medium" (40-70%), or "low" (<40%)
3. List 2-3 specific things to check to validate this hypothesis
4. Keep it practical and actionable for a product manager

Return ONLY a JSON array of hypotheses in this exact format:
[
  {{
    "hypothesis": "Clear statement of what might be happening",
    "confidence": "high|medium|low",
    "supporting_evidence": ["stat fact 1", "stat fact 2"],
    "checks_to_validate": ["check 1", "check 2", "check 3"]
  }}
]"""

        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "system",
                        "content": "You are an expert product analyst who generates data-driven hypotheses based on statistical findings."
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=0.7,
                max_tokens=2000,
                response_format={"type": "json_object"}
            )

            # Parse response
            content = response.choices[0].message.content
            hypotheses_data = json.loads(content)

            # Handle both array and object with 'hypotheses' key
            if isinstance(hypotheses_data, dict) and "hypotheses" in hypotheses_data:
                hypotheses_list = hypotheses_data["hypotheses"]
            elif isinstance(hypotheses_data, list):
                hypotheses_list = hypotheses_data
            else:
                hypotheses_list = []

            # Convert to Hypothesis objects
            hypotheses = []
            for h in hypotheses_list[:max_hypotheses]:
                hypotheses.append(Hypothesis(
                    hypothesis=h.get("hypothesis", ""),
                    confidence=h.get("confidence", "medium"),
                    supporting_evidence=h.get("supporting_evidence", []),
                    checks_to_validate=h.get("checks_to_validate", [])
                ))

            return hypotheses

        except Exception as e:
            print(f"Warning: LLM hypothesis generation failed: {e}")
            return []

    def generate_executive_summary(
        self,
        report: AnalysisReport
    ) -> str:
        """
        Generate a concise executive summary for PMs.

        Args:
            report: Complete analysis report

        Returns:
            Natural language executive summary
        """
        context = self._prepare_analysis_context(report)

        prompt = f"""You are writing an executive summary for a product manager. Summarize the key findings in 3-4 sentences.

ANALYSIS CONTEXT:
{context}

Write a clear, concise executive summary that:
1. Highlights the most important trend or finding
2. Mentions any critical changes or anomalies
3. Suggests the top priority action
4. Uses simple, non-technical language

Return ONLY the summary text (no JSON, no formatting)."""

        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "system",
                        "content": "You are a product analytics expert who writes clear, actionable executive summaries."
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=0.5,
                max_tokens=500
            )

            return response.choices[0].message.content.strip()

        except Exception as e:
            print(f"Warning: LLM summary generation failed: {e}")
            return "Executive summary generation failed. See detailed findings below."

    def generate_recommendations(
        self,
        report: AnalysisReport,
        max_recommendations: int = 3
    ) -> List[RecommendedDecision]:
        """
        Generate actionable recommendations using LLM.

        Args:
            report: Complete analysis report
            max_recommendations: Maximum number of recommendations

        Returns:
            List of recommended decisions with rationale
        """
        context = self._prepare_analysis_context(report)

        prompt = f"""Based on this product metrics analysis, recommend {max_recommendations} specific decisions or actions a product manager should take.

ANALYSIS CONTEXT:
{context}

For each recommendation:
1. State the specific decision or action
2. Explain the rationale based on the data
3. Estimate impact as "high", "medium", or "low"
4. Note any risks or caveats
5. Make it actionable and specific

Return ONLY a JSON array in this exact format:
[
  {{
    "decision": "Specific action to take",
    "rationale": "Why this makes sense based on the data",
    "impact": "high|medium|low",
    "risks": ["risk 1", "risk 2"]
  }}
]"""

        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "system",
                        "content": "You are a senior product manager who makes data-driven decisions."
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=0.6,
                max_tokens=1500,
                response_format={"type": "json_object"}
            )

            content = response.choices[0].message.content
            recommendations_data = json.loads(content)

            # Handle both array and object with 'recommendations' key
            if isinstance(recommendations_data, dict) and "recommendations" in recommendations_data:
                recs_list = recommendations_data["recommendations"]
            elif isinstance(recommendations_data, list):
                recs_list = recommendations_data
            else:
                recs_list = []

            recommendations = []
            for r in recs_list[:max_recommendations]:
                recommendations.append(RecommendedDecision(
                    decision=r.get("decision", ""),
                    rationale=r.get("rationale", ""),
                    impact=r.get("impact", "medium"),
                    risks=r.get("risks", [])
                ))

            return recommendations

        except Exception as e:
            print(f"Warning: LLM recommendations generation failed: {e}")
            return []

    def _prepare_analysis_context(self, report: AnalysisReport) -> str:
        """Prepare a concise context string from the analysis report."""

        context_parts = []

        # Data overview
        context_parts.append(f"DATA: {report.data_profile.row_count} rows, {report.data_profile.column_count} columns")

        # KPIs detected
        if report.kpis_detected:
            kpis = ", ".join([k.column_name for k in report.kpis_detected[:5]])
            context_parts.append(f"KPIs: {kpis}")

        # Top trends
        if report.overall_trends:
            trends_summary = []
            for trend in report.overall_trends[:3]:
                direction = "↗️ up" if trend.direction == "increasing" else "↘️ down"
                trends_summary.append(
                    f"{trend.metric} {direction} {trend.percent_change:+.1f}%"
                )
            context_parts.append("TRENDS: " + "; ".join(trends_summary))

        # Change points
        if report.change_points:
            changes_summary = []
            for cp in report.change_points[:3]:
                direction = "jump" if cp.magnitude > 0 else "drop"
                changes_summary.append(
                    f"{cp.metric} {direction} of {abs(cp.magnitude):.1f}% on {cp.date}"
                )
            context_parts.append("CHANGES: " + "; ".join(changes_summary))

        # Experiments
        if report.experiment_results:
            exp_summary = []
            for exp in report.experiment_results[:2]:
                sig = "significant" if exp.is_significant else "not significant"
                exp_summary.append(
                    f"{exp.metric}: {exp.uplift_percent:+.1f}% uplift ({sig})"
                )
            context_parts.append("EXPERIMENTS: " + "; ".join(exp_summary))

        # Segment drivers
        if report.segment_drivers:
            seg_summary = []
            for seg in report.segment_drivers[:2]:
                seg_summary.append(
                    f"{seg.segment_name} drives {seg.percent_contribution:.1f}% of change"
                )
            context_parts.append("SEGMENTS: " + "; ".join(seg_summary))

        return "\n".join(context_parts)


def is_llm_available() -> bool:
    """Check if LLM integration is available."""
    return OPENAI_AVAILABLE and bool(os.getenv("OPENAI_API_KEY"))


def create_llm_generator(
    api_key: Optional[str] = None,
    model: str = "gpt-4-turbo-preview"
) -> Optional[LLMInsightGenerator]:
    """
    Create an LLM generator if available.

    Returns None if OpenAI is not configured.
    """
    try:
        return LLMInsightGenerator(api_key=api_key, model=model)
    except (ImportError, ValueError):
        return None
